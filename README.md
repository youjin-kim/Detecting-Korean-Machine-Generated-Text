# Detecting-Korean-Machine-Generated-Text

This research investigates the use of a KoBERT model, a BERT version suitable for the Korean language, in order to detect machine-generated text. The KoBERT model and other specific models are required for NLP tasks because of the complicated and agglutinative character of Korean. Among them, the KoBERT model was utilised in the study. In order to evaluate KoBERT's text categorization performance, 80,000 samples from AI-Hub, which is operated by the Korean government, were used in the study. The data were split evenly between texts created by machines and humans. It was shown to be critical to measure accuracy and loss throughout training. Accuracy gives a clear indication of how well a prediction worked, while loss functions provide information about how well the model predicted actual results, which helps with optimisation. By avoiding overfitting, this dual monitoring strategy makes sure the model can continue to generalise to unseen data. Additionally, precision, recall, and F1 score were additional important performance indicators that were employed to assess the model. In order to avoid overfitting, the model was trained with an early stopping mechanism that would terminate training after 2 consecutive epochs in which validation loss did not improve. KoBERT displayed 99.41% accuracy on the training set and 99.29% accuracy on the validation set, according to the final findings. The performance measures demonstrated remarkable results: 98.69% recall, 99.89% precision, and a 99.28% F1 score. Overall, these measurements highlight how well KoBERT performs at differentiating between language generated by machines and text authored by humans, and this result also emphasises the need for specific models for languages with distinct characteristics. 
